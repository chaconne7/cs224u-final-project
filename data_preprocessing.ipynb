{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from constants import *\n",
    "import json \n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_indices(answers): \n",
    "    mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "    return [mapping[entry] for entry in answers]\n",
    "    \n",
    "def fill_in_extra_blanks(sentence, answer_indices, options, blank_index): \n",
    "    index = sentence.find('_');\n",
    "    num_blanks = sentence.count('_')\n",
    "    replacements = []\n",
    "    for i in range(num_blanks): \n",
    "        word = options[blank_index + i][answer_indices[blank_index + i]] if i > 0 else 'MASK'\n",
    "        replacements.append(word)\n",
    "        \n",
    "    def replace_with(_):\n",
    "        \"\"\"Returns first value of substr and removes it.\"\"\"\n",
    "        return replacements.pop(0)\n",
    "    \n",
    "    sentence = re.sub(\"_\",replace_with,sentence)\n",
    "\n",
    "    return sentence, num_blanks\n",
    "    \n",
    "def create_training_example(sentence, options_for_sentence, correct_answer): \n",
    "    example = {}\n",
    "    sentence = \" \".join(sentence.split())\n",
    "    example['sentence'] = sentence \n",
    "    example['answer'] = correct_answer \n",
    "    example['candidates'] = options_for_sentence\n",
    "    return example\n",
    "    \n",
    "    \n",
    "def create_processed_file(data_split, data_type, source_filename): \n",
    "    source_filepath = RAW_PATH + data_split + data_type + source_filename \n",
    "    dest_filename = source_filename[:-4] + 'pickle'\n",
    "    dest_filepath = CLEANED_PATH + data_split + data_type + dest_filename\n",
    "    with open(source_filepath, 'r') as json_file, open(dest_filepath, \"wb\") as pickle_file: \n",
    "        examples = []\n",
    "        data = json.load(json_file)\n",
    "        answers = data['answers']\n",
    "        answer_indices = get_answer_indices(answers)\n",
    "        options = data['options']\n",
    "        article = data['article']\n",
    "        blank_index = 0\n",
    "        for sentence in nltk.sent_tokenize(article): \n",
    "            if blank_index >= len(answer_indices): \n",
    "                break\n",
    "            sentence, num_blanks = fill_in_extra_blanks(sentence, answer_indices, options, blank_index)\n",
    "            if num_blanks > 0: \n",
    "                options_for_sentence = options[blank_index]\n",
    "                correct_answer = options_for_sentence[answer_indices[blank_index]]\n",
    "                example = create_training_example(sentence, options_for_sentence, correct_answer)\n",
    "                examples.append(example)\n",
    "            blank_index += num_blanks \n",
    "        pickle.dump(examples, pickle_file)\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_files(): \n",
    "    for data_split in DATA_SPLITS: \n",
    "        for data_type in DATA_TYPES:\n",
    "            path = RAW_PATH + data_split + data_type \n",
    "            filenames = os.listdir(path)\n",
    "            for filename in filenames: \n",
    "                filename_path = path + filename\n",
    "                create_processed_file(data_split, data_type, filename)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sentence': 'My heart MASK when I was asked to the back room by the immigration officer.', 'answer': 'sank', 'candidates': ['ached', 'beat', 'sank', 'rose']}, {'sentence': 'My MASK , with his very American last name, had no trouble at all.', 'answer': 'husband', 'candidates': ['son', 'daughter', 'friend', 'husband']}, {'sentence': \"In fact, I am MASK American born and raised, but they weren't quite ready to let me in yet.\", 'answer': 'also', 'candidates': ['still', 'also', 'already', 'never']}, {'sentence': \"The only reason was MASK they thought my name looked like the one of someone who's on their wanted list and I had to wait till they checked me out with Washington.\", 'answer': 'that', 'candidates': ['that', 'because', 'why', 'whether']}, {'sentence': 'Time passed MASK .', 'answer': 'slowly', 'candidates': ['quickly', 'carefully', 'dangerously', 'slowly']}, {'sentence': 'One hour, one hour and a half...I could not wait any longer and MASK my cellphone out to call the friend I had planned to meet that evening.', 'answer': 'pulled', 'candidates': ['put', 'pulled', 'used', 'caught']}, {'sentence': 'An officer MASK over.', 'answer': 'rushed', 'candidates': ['went', 'came', 'cried', 'rushed']}, {'sentence': 'he said, \"For all we know you could be calling terrorists and giving them MASK .\"', 'answer': 'information', 'candidates': ['news', 'truth', 'information', 'reply']}, {'sentence': 'I had no MASK but to put my phone away.', 'answer': 'choice', 'candidates': ['response', 'voice', 'choice', 'face']}, {'sentence': 'My husband and I were getting hungry and MASK .', 'answer': 'tired', 'candidates': ['silent', 'tired', 'comfortable', 'clear']}, {'sentence': 'I wanted to cry, to MASK onto a chair and shout: \"I am but an American professor!\"', 'answer': 'jump', 'candidates': ['sit', 'run', 'jump', 'lie']}, {'sentence': \"After two hours in the back room, without explanation and MASK , I was allowed to go after he gave me a piece of paper with a(n) address on it and told me I could write to the department if I wasn't happy with the treatment.\", 'answer': 'apologies', 'candidates': ['expressions', 'words', 'thanks', 'apologies']}, {'sentence': 'He also MASK that nothing could stop it from happening again.', 'answer': 'added', 'candidates': ['added', 'spoke', 'talked', 'argued']}, {'sentence': 'I shared my experience with my friends and the MASK was I should change my name.', 'answer': 'advice', 'candidates': ['advice', 'result', 'way', 'agreement']}, {'sentence': \"Even though I had a troublesome experience at the airport, which made me realize being American could ever be so MASK , like my father, I'll keep the name .\", 'answer': 'hard', 'candidates': ['easy', 'long', 'hard', 'high']}]\n"
     ]
    }
   ],
   "source": [
    "with open('data/cleaned/train/high/high0.pickle', 'rb') as pickle_file:\n",
    "    content = pickle.load(pickle_file)\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
